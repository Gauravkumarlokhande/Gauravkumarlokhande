{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "luhn algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN337RJlJBu8hVK8OCismCb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gauravkumarlokhande/Gauravkumarlokhande/blob/main/luhn_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pVTBAQu_bE3z"
      },
      "outputs": [],
      "source": [
        "import re  #regular expression library\n",
        "import nltk # natural language toolkit\n",
        "import string\n",
        "import heapq #to order the sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in order to work with words and sentence tokenizer, download punkt"
      ],
      "metadata": {
        "id": "wBfYalsNb5EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVOXivg_bOQw",
        "outputId": "2f5407b3-72c1-498e-b5d5-97116838602f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To work with stop words download stopwords "
      ],
      "metadata": {
        "id": "wU_L25eScDWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqhKQKPrbUfh",
        "outputId": "6b1238f3-3f3e-4ad9-e08b-6be44f73addb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original = \"\"\"Artificial intelligence is human like intelligence machines.\n",
        "              It is the study of intelligent artificial agents.\n",
        "              Science and engineering to produce intelligent machines.\n",
        "              Solve problems and have intelligence.\n",
        "              Related to intelligent behavior.\n",
        "              Developing of reasoning machines.\n",
        "              Learn from mistakes and successes.\n",
        "              Artificial intelligence is related to reasing in everyday situations.\"\"\"\n",
        "              \n"
      ],
      "metadata": {
        "id": "R6INJLq8bZBo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "BucRp7iOdHI-",
        "outputId": "eaed8bc9-62f3-4579-80ee-074aa711252d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial intelligence is human like intelligence machines.\\n              It is the study of intelligent artificial agents.\\n              Science and engineering to produce intelligent machines.\\n              Solve problems and have intelligence.\\n              Related to intelligent behavior.\\n              Developing of reasoning machines.\\n              Learn from mistakes and successes.\\n              Artificial intelligence is related to reasing in everyday situations.'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have many extra spaces here. so to remove the extra spaces we write the following code and just replace the big spaces by small spaces."
      ],
      "metadata": {
        "id": "Xruoigu3dTiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original=re.sub(r'\\s+',\" \",original) # the + tells that there may another spaces in the passage which allows the command to go through whole passage"
      ],
      "metadata": {
        "id": "4VBcF1GFdOi_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nZKsYx4Mdv80",
        "outputId": "42474cf5-ad82-4b55-86de-2194435fe255"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial intelligence is human like intelligence machines. It is the study of intelligent artificial agents. Science and engineering to produce intelligent machines. Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines. Learn from mistakes and successes. Artificial intelligence is related to reasing in everyday situations.'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we need to check for the stopwords in english language so we use the following command."
      ],
      "metadata": {
        "id": "NUnZLOFBd85H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop=nltk.corpus.stopwords.words('english')\n",
        "print(stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQeSedxHdxEG",
        "outputId": "80b36075-b8e8-4ed2-e62a-dd74341a9bb0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a function to tokenize the words and convert to lower case and remosve stopwords"
      ],
      "metadata": {
        "id": "oqrR3iJrfhdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process(text):\n",
        "  new_text=text.lower()  # covert all the words in to lower case\n",
        "  tokens=[]  #create a empty list by name tokens\n",
        "  for token in nltk.word_tokenize(new_text):\n",
        "    tokens.append(token)     # append each tokenized word in the empty list\n",
        "  tokens=[word for word in tokens if word not in stop and word not in string.punctuation]   #we remove the stopwords and punctuation mark\n",
        "  new_text=\" \".join(element for element in tokens) # join all the words in to a from of sentence which are in a form of list\n",
        "  return new_text\n"
      ],
      "metadata": {
        "id": "sM5rnsCkePAn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text=process(original)\n",
        "new_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "eKlXyn8_hPl0",
        "outputId": "17816281-45f2-4a28-c8a5-e182c9506528"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'artificial intelligence human like intelligence machines study intelligent artificial agents science engineering produce intelligent machines solve problems intelligence related intelligent behavior developing reasoning machines learn mistakes successes artificial intelligence related reasing everyday situations'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating the sentence score** "
      ],
      "metadata": {
        "id": "UTmBoW-v52pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sent_score(sentences,important_words,distance): # the second parameter is to return the top 5 important words\n",
        "  scores=[]  # empty list\n",
        "  sentence_index=0\n",
        "  for sentence in [nltk.word_tokenize(sentence) for sentence in sentences]: # gives list of words in every sentence\n",
        "    #print('-------------')\n",
        "    #print(sentence)  # gives each sentence in a list form\n",
        "    word_index=[]  # empty list to ente rthe index\n",
        "    for word in important_words:\n",
        "      #print(word) \n",
        "      try:\n",
        "        word_index.append(sentence.index(word)) # add the index of the word which is present in the list\n",
        "      #now when the word is not there in the list it will return error\n",
        "      except ValueError:\n",
        "        pass           # this will avoid the value for the word that are not present in the list and go for another word\n",
        "    word_index.sort()\n",
        "    #print(word_index)\n",
        "    if len(word_index)==0:  #if the difference between the indexes is 0 then we skip that sentence\n",
        "      continue\n",
        "    group_list=[]\n",
        "    group=[word_index[0]]   #to return word index at position 0\n",
        "    # [1,2,3]\n",
        "    i=1\n",
        "    while i<len(word_index):    # to go through each word index in each sentence   # lenght is 3\n",
        "      if word_index[i]-word_index[i-1]<distance: # here i=1 and i-1=0  # if the difference is less than distance then we execute the loop\n",
        "        group.append(word_index[i])\n",
        "      else:\n",
        "        group_list.append(group[:])   # or else the distance is not less than the difference then we store the index in group_list\n",
        "        group=[word_index[i]]\n",
        "        #print('group',group)\n",
        "      i+=1                          # when the value of index i is not less then the length of word index \n",
        "      group_list.append(group)\n",
        "      #print('all_groups',group_list)\n",
        "      max_group_score=0\n",
        "      for g in group_list:\n",
        "        #print(g)\n",
        "        important_words_in_group=len(g)\n",
        "        total_words_in_group=g[-1]-g[0]+1\n",
        "        score=1.0*important_words_in_group**2/total_words_in_group\n",
        "        #print('group_score',score)\n",
        "\n",
        "        if score>max_group_score:\n",
        "          max_group_score=score\n",
        "\n",
        "      scores.append((max_group_score,sentence_index))\n",
        "      sentence_index+=1\n",
        "    \n",
        "  #print('final score',scores)\n",
        "  return scores\n",
        "\n",
        "  \n",
        "\n",
        "     \n",
        "         \n",
        "\n"
      ],
      "metadata": {
        "id": "Cp85wIIP50Y8"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Summarizing the text***"
      ],
      "metadata": {
        "id": "oGANs4fBiKG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, top_n_words,distance,number_of_sentences):\n",
        "  original_sent=[sentence for sentence in nltk.sent_tokenize(text)]                         #tokenize the whole sentence and store in a list\n",
        "  formatted_sentence=[process(original_sentence) for original_sentence in original_sent ]    # to apply the whole procedure above on just sentences.\n",
        "  words=[word for sentence in formatted_sentence for word in nltk.word_tokenize(sentence)]   # tokenize the sentences in the form of words\n",
        "  frequency=nltk.FreqDist(words)                                                             # calculate the frequency for each word (how many times repeated. more the frequency, more is the word imp)\n",
        "  top_n_words=[word[0] for word in frequency.most_common(top_n_words)]                       # to return top good frequency words\n",
        "  sentences_score=calculate_sent_score(formatted_sentence,top_n_words,distance)\n",
        "  #print(sentences_score)\n",
        "  best_sentences=heapq.nlargest(number_of_sentences,sentences_score)\n",
        "  #print(best_sentences)\n",
        "  best_sentences=[original_sent[i] for (score,i) in best_sentences]\n",
        "  #print(best_sentences)\n",
        "  return best_sentences, original_sent, sentences_score\n"
      ],
      "metadata": {
        "id": "3N_zQPWwhWGI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_sentences, original_sent, sentences_score=summarize(original,5,2,3)"
      ],
      "metadata": {
        "id": "tMBq-TgzonST"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here in final score: (1.0,2) means sentence 2 has score 1.0"
      ],
      "metadata": {
        "id": "7_yT0t21SedA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_sentences"
      ],
      "metadata": {
        "id": "DyHYfku-osfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e77a18-333f-41a4-f5a6-b0f1ff905daf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Learn from mistakes and successes.',\n",
              " 'Developing of reasoning machines.',\n",
              " 'Related to intelligent behavior.']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh-sIzPzWyDc",
        "outputId": "bfedf8fe-279b-4b5f-ad96-34a798f9da27"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2.0, 0), (2.0, 1), (2.0, 2), (2.0, 3), (2.0, 4), (2.0, 5), (3.0, 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***implementation of the code***"
      ],
      "metadata": {
        "id": "x4HCPJg3XGF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML"
      ],
      "metadata": {
        "id": "m7NqLYtcXEV5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=''\n",
        "display(HTML(f'<h2>summary</h2>'))\n",
        "for sentence in original_sent:\n",
        "  if sentence in best_sentences:\n",
        "    text+=''+sentence.replace(sentence,f\"<mark>{sentence}</mark>\")\n",
        "  else:\n",
        "    text+=''+sentence\n",
        "display(HTML(f\"\"\"{text}\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "VjEMG11fXe_P",
        "outputId": "95c11c68-2849-4e98-ffed-6fed6da26812"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>summary</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Artificial intelligence is human like intelligence machines.It is the study of intelligent artificial agents.Science and engineering to produce intelligent machines.Solve problems and have intelligence.<mark>Related to intelligent behavior.</mark><mark>Developing of reasoning machines.</mark><mark>Learn from mistakes and successes.</mark>Artificial intelligence is related to reasing in everyday situations."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WD-7P57EYo0O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
